{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d9590a31",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e4a91a08",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nRDD \\nmap\\nfilter\\nflatfilter\\nreduce (agg)\\ngroup\\njoin\\ngroup avg\\n'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "RDD \n",
    "map\n",
    "filter\n",
    "flatfilter\n",
    "reduce (agg)\n",
    "group\n",
    "join\n",
    "group avg\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fa1ef77",
   "metadata": {},
   "source": [
    "#### RDD Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "415fef73",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b4c3b33f",
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2fec99e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd1 = sc.parallelize([1,2,3,4,5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5c4199dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 3, 4, 5]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd1.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d8930694",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.arange(1,50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "95c4f6f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n",
       "       18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34,\n",
       "       35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9df0e895",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 3, 4, 5]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd2 = sc.parallelize(data)\n",
    "rdd2.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "458bf0aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fun1 (x): \n",
    "    return x**3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "905c8baa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 8, 27, 64, 125]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd3 = rdd2.map(fun1)\n",
    "rdd3.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "81409f56",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 8, 27, 64, 125]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd3 = rdd2.map(lambda x: x**3)\n",
    "rdd3.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d2c562d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd4 = rdd3.map(lambda x: x + 5)\n",
    "rdd5 = rdd3.map(lambda x: x / 5)\n",
    "rdd6 = rdd3.map(lambda x: x - 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5ba696fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[-49, -42, -23, 14, 75]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd6.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c6aa64a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[-48.8, -47.4, -43.6, -36.2, -24.0]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd7 = rdd3.map(lambda x: x + 5).map(lambda x: x / 5).map(lambda x: x - 50)\n",
    "rdd7.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d6973be5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[-48.8, -47.4, -43.6, -36.2, -24.0]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd7 = rdd3.map(lambda x: x + 5)\\\n",
    ".map(lambda x: x / 5)\\\n",
    ".map(lambda x: x - 50)\n",
    "rdd7.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2c65c5b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[-48.8, -47.4, -43.6, -36.2, -24.0]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd7 = (rdd3.map(lambda x: x + 5)\n",
    "        .map(lambda x: x / 5)\n",
    "        .map(lambda x: x - 50))\n",
    "rdd7.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2e95707f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2, 4, 6, 8, 10]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd8 = rdd2.filter(lambda x: x % 2 == 0)\n",
    "rdd8.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "48638e05",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 11, 13, 17, 19]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd9 = rdd2.filter(lambda x: x % 2 != 0 and x % 3 != 0 and x % 5 != 0 and x % 7 != 0)\n",
    "rdd9.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "194c6082",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1225"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd10 = rdd2.reduce(lambda x, y: x + y)\n",
    "rdd10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "60827343",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd2.min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7f20ff5c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14.142135623730951"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd2.stdev()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a86edfa8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200.0"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd2.variance()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "445a9078",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "49"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd2.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "32068e8b",
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\n: org.apache.hadoop.mapred.InvalidInputException: Input path does not exist: file:/shared/egypt.txt\n\tat org.apache.hadoop.mapred.FileInputFormat.singleThreadedListStatus(FileInputFormat.java:304)\n\tat org.apache.hadoop.mapred.FileInputFormat.listStatus(FileInputFormat.java:244)\n\tat org.apache.hadoop.mapred.FileInputFormat.getSplits(FileInputFormat.java:332)\n\tat org.apache.spark.rdd.HadoopRDD.getPartitions(HadoopRDD.scala:208)\n\tat org.apache.spark.rdd.RDD.$anonfun$partitions$2(RDD.scala:291)\n\tat scala.Option.getOrElse(Option.scala:189)\n\tat org.apache.spark.rdd.RDD.partitions(RDD.scala:287)\n\tat org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:49)\n\tat org.apache.spark.rdd.RDD.$anonfun$partitions$2(RDD.scala:291)\n\tat scala.Option.getOrElse(Option.scala:189)\n\tat org.apache.spark.rdd.RDD.partitions(RDD.scala:287)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2463)\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1046)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:407)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:1045)\n\tat org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:195)\n\tat org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:568)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:833)\nCaused by: java.io.IOException: Input path does not exist: file:/shared/egypt.txt\n\tat org.apache.hadoop.mapred.FileInputFormat.singleThreadedListStatus(FileInputFormat.java:278)\n\t... 30 more\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[26], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m rddfile \u001b[38;5;241m=\u001b[39m sc\u001b[38;5;241m.\u001b[39mtextFile(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/shared/egypt.txt\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 2\u001b[0m \u001b[43mrddfile\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/rdd.py:1833\u001b[0m, in \u001b[0;36mRDD.collect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1831\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m SCCallSiteSync(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontext):\n\u001b[1;32m   1832\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mctx\u001b[38;5;241m.\u001b[39m_jvm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1833\u001b[0m     sock_info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jvm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mPythonRDD\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollectAndServe\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jrdd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrdd\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1834\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(_load_from_socket(sock_info, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jrdd_deserializer))\n",
      "File \u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/errors/exceptions/captured.py:179\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    177\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdeco\u001b[39m(\u001b[38;5;241m*\u001b[39ma: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m    178\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 179\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    180\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    181\u001b[0m         converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
      "File \u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[0;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[1;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[1;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[1;32m    331\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    332\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\n: org.apache.hadoop.mapred.InvalidInputException: Input path does not exist: file:/shared/egypt.txt\n\tat org.apache.hadoop.mapred.FileInputFormat.singleThreadedListStatus(FileInputFormat.java:304)\n\tat org.apache.hadoop.mapred.FileInputFormat.listStatus(FileInputFormat.java:244)\n\tat org.apache.hadoop.mapred.FileInputFormat.getSplits(FileInputFormat.java:332)\n\tat org.apache.spark.rdd.HadoopRDD.getPartitions(HadoopRDD.scala:208)\n\tat org.apache.spark.rdd.RDD.$anonfun$partitions$2(RDD.scala:291)\n\tat scala.Option.getOrElse(Option.scala:189)\n\tat org.apache.spark.rdd.RDD.partitions(RDD.scala:287)\n\tat org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:49)\n\tat org.apache.spark.rdd.RDD.$anonfun$partitions$2(RDD.scala:291)\n\tat scala.Option.getOrElse(Option.scala:189)\n\tat org.apache.spark.rdd.RDD.partitions(RDD.scala:287)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2463)\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1046)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:407)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:1045)\n\tat org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:195)\n\tat org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:568)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:833)\nCaused by: java.io.IOException: Input path does not exist: file:/shared/egypt.txt\n\tat org.apache.hadoop.mapred.FileInputFormat.singleThreadedListStatus(FileInputFormat.java:278)\n\t... 30 more\n"
     ]
    }
   ],
   "source": [
    "rddfile = sc.textFile(\"/shared/egypt.txt\")\n",
    "rddfile.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ac17584",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rddfile.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be797d90",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['Egypt',\n",
       "  'is',\n",
       "  'a',\n",
       "  'country',\n",
       "  'in',\n",
       "  'North',\n",
       "  'Africa',\n",
       "  'known',\n",
       "  'for',\n",
       "  'its',\n",
       "  'ancient',\n",
       "  'civilization',\n",
       "  'and',\n",
       "  'iconic',\n",
       "  'monuments',\n",
       "  'such',\n",
       "  'as',\n",
       "  'the',\n",
       "  'Pyramids',\n",
       "  'of',\n",
       "  'Giza',\n",
       "  'and',\n",
       "  'the',\n",
       "  'Sphinx.'],\n",
       " ['The',\n",
       "  'capital',\n",
       "  'city,',\n",
       "  'Cairo,',\n",
       "  'is',\n",
       "  'the',\n",
       "  'largest',\n",
       "  'in',\n",
       "  'the',\n",
       "  'Arab',\n",
       "  'world',\n",
       "  'and',\n",
       "  'home',\n",
       "  'to',\n",
       "  'the',\n",
       "  'Nile',\n",
       "  'River,',\n",
       "  'the',\n",
       "  'longest',\n",
       "  'river',\n",
       "  'in',\n",
       "  'the',\n",
       "  'world.'],\n",
       " ['Egypt',\n",
       "  'has',\n",
       "  'a',\n",
       "  'rich',\n",
       "  'cultural',\n",
       "  'heritage',\n",
       "  'blending',\n",
       "  'Pharaonic,',\n",
       "  'Greco-Roman,',\n",
       "  'Islamic,',\n",
       "  'and',\n",
       "  'modern',\n",
       "  'influences.'],\n",
       " ['Tourism,',\n",
       "  'agriculture,',\n",
       "  'and',\n",
       "  'the',\n",
       "  'Suez',\n",
       "  'Canal',\n",
       "  'are',\n",
       "  'major',\n",
       "  'contributors',\n",
       "  'to',\n",
       "  'its',\n",
       "  'economy.']]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd_spited = rddfile.map(lambda line: line.split())\n",
    "rdd_spited.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e368e95e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "72"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd_spited = rddfile.flatMap(lambda line: line.split())\n",
    "rdd_spited.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64cf8927",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[True, False, True, False]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd_egypt = rddfile.map(lambda line: 'Egypt' in line)\n",
    "rdd_egypt.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23566d3f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Egypt is a country in North Africa known for its ancient civilization and iconic monuments such as the Pyramids of Giza and the Sphinx. ',\n",
       " 'Egypt has a rich cultural heritage blending Pharaonic, Greco-Roman, Islamic, and modern influences. ']"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd_egypt = rddfile.filter(lambda line: 'Egypt' in line)\n",
    "rdd_egypt.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcaf037a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[24, 23, 13, 12]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd_count_words = rddfile.map(lambda line: len(line.split()))\n",
    "rdd_count_words.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "449a0574",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd_egypt_count_words = rddfile.flatMap(lambda line: line.split())\\\n",
    "    .filter(lambda word: word == 'Egypt')\\\n",
    "    .count()\n",
    "rdd_egypt_count_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8c51947",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Ahmed', 'Mona', 'Shahd', 'Shahd', 'Khaled']"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "names = ['Ahmed', 'Mona', 'Shahd', 'Shahd', 'Khaled']\n",
    "rdd_names = sc.parallelize(names)\n",
    "rdd_names.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d38cdab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('S', <pyspark.resultiterable.ResultIterable at 0x7f8c00673550>),\n",
       " ('A', <pyspark.resultiterable.ResultIterable at 0x7f8be56e4d50>),\n",
       " ('M', <pyspark.resultiterable.ResultIterable at 0x7f8be5825090>),\n",
       " ('K', <pyspark.resultiterable.ResultIterable at 0x7f8c004e2710>)]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd_groups = rdd_names.groupBy(lambda word: word[0])\n",
    "rdd_groups.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14a2078f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('S', <pyspark.resultiterable.ResultIterable at 0x7f8c00574650>)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd_groups.collect()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17f877b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Shahd', 'Shahd']"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(rdd_groups.collect()[0][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7665037",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('S', ['Shahd', 'Shahd']),\n",
       " ('A', ['Ahmed']),\n",
       " ('M', ['Mona']),\n",
       " ('K', ['Khaled'])]"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "groups_names = [(k,list(v)) for k,v in rdd_groups.collect()]\n",
    "groups_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87a917e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('a', (5, 7)), ('b', (6, 10)), ('b', (10, 10))]"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd_num1 = sc.parallelize([('a',5) , ('b',6), ('b',10)])\n",
    "rdd_num2 = sc.parallelize([('a',7) , ('b',10), ('c',9)])\n",
    "rdd_join = rdd_num1.join(rdd_num2)\n",
    "rdd_join.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "159775ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "student_data = [('Ahmed',25), ('Mona',30), ('Shahd',17), ('Shahd',22), ('Mona',16)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d194569",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Ahmed', 25), ('Mona', 30), ('Shahd', 17), ('Shahd', 22), ('Mona', 16)]"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd_student = sc.parallelize(student_data)\n",
    "rdd_student.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb4d3319",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Mona', <pyspark.resultiterable.ResultIterable at 0x7f8bdd230310>),\n",
       " ('Ahmed', <pyspark.resultiterable.ResultIterable at 0x7f8bdd230490>),\n",
       " ('Shahd', <pyspark.resultiterable.ResultIterable at 0x7f8be56c0810>)]"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd_student1 = rdd_student.groupByKey()\n",
    "rdd_student1.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dab86cd4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Mona', [30, 16]), ('Ahmed', [25]), ('Shahd', [17, 22])]"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "student_list= [(k,list(v)) for k,v in rdd_student1.collect()]\n",
    "student_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15ffdf32",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Mona', 2), ('Ahmed', 1), ('Shahd', 2)]"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "student_list= [(k,len(list(v))) for k,v in rdd_student1.collect()]\n",
    "student_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87fec27e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Ahmed', (25, 1)),\n",
       " ('Mona', (30, 1)),\n",
       " ('Shahd', (17, 1)),\n",
       " ('Shahd', (22, 1)),\n",
       " ('Mona', (16, 1))]"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd_student1 = rdd_student.map(lambda x: (x[0],(x[1],1)))\n",
    "rdd_student1.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdcda852",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Mona', (46, 2)), ('Ahmed', (25, 1)), ('Shahd', (39, 2))]"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd_student2 = rdd_student1.reduceByKey(lambda x , y: (x[0] + y[0],x[1] + y[1]))\n",
    "rdd_student2.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35c4b09a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Mona', 23.0), ('Ahmed', 25.0), ('Shahd', 19.5)]"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd_student3 = rdd_student2.map(lambda x: (x[0], x[1][0] / x[1][1]))\n",
    "rdd_student3.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "163ade5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---+\n",
      "| Name|Age|\n",
      "+-----+---+\n",
      "|Ahmed| 25|\n",
      "| Mona| 30|\n",
      "|Shahd| 17|\n",
      "|Shahd| 22|\n",
      "| Mona| 16|\n",
      "+-----+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_student = spark.createDataFrame(student_data, [\"Name\", \"Age\"])\n",
    "df_student.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82b9d370",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------+\n",
      "| Name|avg(Age)|\n",
      "+-----+--------+\n",
      "|Ahmed|    25.0|\n",
      "| Mona|    23.0|\n",
      "|Shahd|    19.5|\n",
      "+-----+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_student_avg = df_student.groupBy(\"Name\").avg(\"Age\")\n",
    "df_student_avg.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
